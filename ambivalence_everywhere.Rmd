---
title: Ambivalence is Everywhere&#58; Quantifying Attitude Stability Across Topic Domains[^thanks]
author:
  - Kevin Kiley, Duke University[^kk]
date: "1/13/2021"
output: 
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: false
    number_sections: true
bibliography: ["kileybib.bib"]
header-includes:
   - \usepackage{setspace}\onehalfspacing
mainfont: Minion Pro
fontsize: 11pt
abstract: A major debate in the social sciences centers on whether people hold consistent attitudes over time or whether attitudes are temporary constructs. A middle ground suggests that stable opinions are a function of social structure and attention, and on any particular issue, some people hold stable attitudes and others do not. This paper uses a finite mixture model approach to quantify the proportion of people who hold stable attitudes, the proportion of people who make durable changes, and the proportion of people who demonstrate vacillating or ambivalent responses for more than 500 survey questions across 10 panel data sets. The results suggest wide variation across questions in the proportion of respondents who hold stable attitudes, with most subject areas demonstrating high levels of inconsistency. Stability is also socially patterned, with people demonstrating stability on related issues, suggesting that over-time instability in responses is not measurement error, but that the general public is divided into "issue publics" that have stable opinions on different issues. Rather than argue that people in general hold or lack opinions, the results show that stable opinions are socially contingent.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(broom)
library(knitr)
library(ggbeeswarm)
library(ggrepel)
library(igraph)
source("~/Dropbox/ambivalence_everywhere/functions/model_function.R")
load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/results_df.Rdata")
load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/marhomo_model.Rdata")
qm <- read_csv("~/Dropbox/ambivalence_everywhere/questionmacro.csv")
```

[^thanks]: Thanks to Stephen Vaisey, Craig Rawlings, and Nicholas Restrepo Ochoa for feedback on early drafts. Thanks to Jennifer Hill for guidance on implementing the model and for providing additional resources. Thanks to Christopher Johnston for originally pointing me to the fininte mixture model approach.
[^kk]: Ph.D. Candidate, Department of Sociology, Duke University, <kevin.kiley@duke.edu>.

# Introduction

What proportion of the population holds stable attitudes? This question has been a key debate in the social sciences since Philip Converse claimed that "large portions of an electorate ... simply do not have meaningful beliefs, even on issues that have formed the basis for intense political controversy among elites for substantial periods of time" [@converse1964]. After decades of research and debate, there is little consensus.

On one hand, researchers point to low wave-to-wave correlations in people's responses to the same question over time [@converse1964], question-order and -wording effects [@perrin2011], the effects of psychological primes, the complexity of culture, and the general limitations of human cognition [@martin2002; @martin2010] to argue that people do not carry around fixed beliefs. Instead, they argue, people are ambivalent on issues of public interest and construct opinions in the interview or survey setting, drawing on recent considerations from their social environment [@zaller1992]. In contrast, other researchers point to the stability of attitudes in the aggregate, both aggregating across questions within people [@ansolabehere2008] and aggregating the same question across a population [@inglehart1985; @page1992]; the infrequency of durable changes in attitudes over time [@kiley2020; @vaisey2016]; and the ability of specific attitudes to predict a range of behaviors over time [@vaisey2009; @vaisey2010] as evidence that people carry at least some stable latent attitudes or dispositions, even if these positions are obscured by measurement error.

Recent work argues that dispositions are mostly stable by the time people reach adulthood [@kiley2020; @vaisey2016; @vaisey2020]. But these findings elide the "non-attitudes" debate by grouping together people who hold stable, unchanging opinions with people who change randomly from wave to wave. In other words, while finding that people tend not to make durable changes in their beliefs over time, they cannot say whether this is because people hold real, stable attitudes or whether it is because they hold no attitudes at all. This is, to put it mildly, an important distinction. 

A middle ground -- going back to Philip Converse's development of the "non-attitudes" thesis -- suggests that on any particular issue (and in any particular window of time) the population comprises three groups: people who hold stable opinions they can report consistently, people who hold weak opinions subject to temporary influences, and a small group of people making durable, real change [@freeder2019; @hill2001a; @zaller1992]. In other words, rather than assuming that all members of the population either have or lack opinions, this approach says "some do, and some do not," a position more consistent with advances in psychology distinguishing between "strong" attitudes, which govern behavior and thought, and weak attitudes, which do not [@howe2017]. But for any particular question, these groups will vary in size and composition. There has been little systematic exploration of which issues demonstrate more or less stability or durable change, especially outside of political attitudes. This has hindered the development of systematic perspectives about the degree to which people form opinions and the social conditions under which they do.

This debate about whether people hold stable attitudes has significant implications for the role of attitudes and beliefs in social behavior. If people do not carry around fixed beliefs, and if their dispositions and behavior are swayed by temporary influences, there is substantial room for contemporary social structures, opinion leaders, and momentary situations to shape attitudes, and explanations for attitudes should be rooted in contemporary social structures, as pragmatist theories suggest [@gross2009; @joas1996]. If people hold stable attitudes that are relatively impervious to momentary changes in social influence, these attitudes would be more likely to shape patterns of behavior, affiliation, and belief over time, and the explanations for attitudes should be located in people's backgrounds, rather than contemporary social structures. A world where, on any particular issue, some people hold a stable attitude and others do not directs attention to "institutions and contexts and other forms of objectified cultural structure" that facilitate attitude stability [@lizardo2010a: p. 206; @martin2010] and suggests that different attitudes might matter in explaining different peoples' behavior. 

In this paper, I use a finite mixture model approach to estimate for more than 500 attitude questions across 10 panel data sets the proportion of respondents who hold stable opinions, the proportion of respondents who hold vacillating attitudes or no opinion at all, and the proportion of respondents who make durable changes of opinion. These questions include topics addressed by political scientists in the past, as well sociologically relevant questions about religion, gender roles, race relations, morality, institutional and social trust, and more.

Across these questions, I find wide variation in the proportion of people who hold stable opinions, with some attitude questions demonstrating widespread response stability across the population (70 percent of respondents or more) while other questions demonstrate rampant inconsistency, with fewer than 20 percent of respondents able to give a consistent opinion over time. Overall, rates of vacillating change tend to exceed rates of stable attitudes, and ambivalence can be found in attitudes about political issues, religious and moral beliefs, self-assessments, sentiment toward groups, and more. People disproportionately demonstrate stability on related issues (e.g., stability on one political issue predicts stability on other political issues, but not on religious issues), suggesting different forces give rise to stability in different domains. Rather than argue that people in general "have" or "lack" opinions, these results suggest both are generally true for any particular question, and they reinforce the argument that stable opinions are principally a function of attention and social structure. 

# Stable Opinions and Non-Attitudes

Broad streams of social science work generally assume that people have relatively stable attitudes or dispositions, which is why Converse's original "non-attitudes" findings generated such a robust debate. 

The most sustained challenge to the "non-attitudes" model in the social sciences are what are called "measurement-error" models, which posit that all people hold (latent) attitudes but report them with some error [@achen1975; @ansolabehere2008; @inglehart1985]. Error arises for a variety of reasons, but when responses to the same question are aggregated at the population level [@page1992], when responses to related questions are aggregated within people [@ansolabehere2008], or certain statistical methods are employed [@inglehart1985; @judd1980], respondents display much higher levels of stability than when we look at their individual responses over time. Researchers interpret this as evidence of stable "latent" attitudes.

In cultural sociology, this line of thinking takes the form of suggesting that people have implicit dispositions that shape their behavior over time, even if they cannot articulate these commitments in interview contexts [@hitlin2004; @vaisey2009]. Vaisey argues that interviews and surveys tap distinct cognitive processes. Interviews tap discursive reasoning, which tends to bring to the surface the breadth of contradictory considerations that people have internalized. Fixed-choice survey questions tend to invoke practical reasoning, gut feelings about which answer is likely correct, which also tends to control social behavior across situations. In support of his argument, he and others show that responses to survey questions about worldviews and values predict a range of behaviors across contexts [@miles2015; @vaisey2009; @vaisey2010] and that quickly assessed questions about the relationship between cultural concepts is predictive of other beliefs [@hunzaker2019]. 

These works give us an expectation that people should be relatively consistent in their cultural commitments over time, especially in survey responses. When asked to give an opinion about an issue, such as whether they agree or disagree that "Morality is a personal matter and society should not force everyone to follow one standard," people might vacillate within a narrow band. On some days, a moral relativist might agree that morals are a personal matter and some days might strongly agree, but when they look at the "lineup" of options, a person with a disposition that morality is a personal matter is not going to say that society should enforce one standard, and they are not going to say they lack a position. These results will "feel" wrong, even if people are not conscious of why.

At the same time, a key finding of cultural sociology in the last half-century is that rather than internalize a consistent cultural worldview through a single socialization process, people are exposed to and internalize a diverse, contradictory cultural repertoire of beliefs, practices, and assessments [@swidler1986; @dimaggio1997]. A long line of research documents this cultural contradiction across domains. In America, love is both a choice entered into freely and a unique and irreplaceable commitment that people cannot leave [@swidler2001]. Morality is fixed and relative [@baker2004], orthodox and progressive [@hunter2000]. American culture is individualist and collectivist; managerial and therapeutic; biblical and republican [@bellah1985]. 

It is not just that culture is diverse and contradictory at the public level -- that concepts compete in the public domain but people maintain distinct worldviews -- but this contradiction is also apparent in personal culture, the declarative and non-declarative attitudes, worldviews, values, and dispositions that manifest at the individual level [@lizardo2017]. Because of their cognitive limitations, people consume a broad array of cultural information and can not or tend not to engage in the effort to reconcile these cultural contradictions [@martin2010; @zaller1992]. As a result, "our heads are full of images, opinions, and information, untagged as to truth value, to which we are inclined to attribute accuracy and plausibility" [@dimaggio1997: p. 267].

Because they have contradictory commitments in their brains, people seem to struggle to maintain their a single line of cultural reasoning over time [@swidler1986]. In successive interviews (and often in the same interview) people can demonstrate very different opinions on the same issue without recognizing these contradictions [@swidler2001]. People draw on different cultural resources to justify institutionally constrained behavior [@mills1940; @scott1968]. And when we look at people's responses to the same attitude question over time, they often demonstrate high levels of inconsistency [@alwin2007; @converse1964; @hout2016].

This work in cultural sociology echoes with a line of public opinion scholarship on how people form opinions in survey settings. In his Receive-Accept-Sample model of opinion formation, Zaller [-@zaller1992; @zaller1992a] argues that the population is characterized by ambivalence toward political issues and tend to be uncritical toward the messages they receive. As a result, they store arguments for "more government spending is good" and "higher taxes are bad" when they hear these messages, and, as Zaller notes, "most of the time, there is no need to reconcile or even recognize their contradictory reactions to events and issues. Each can represent a genuine feeling, capable of coexisting with opposing feelings and, depending on momentary salience in the person's mind, controlling responses to survey questions" [@zaller1992: p. 93]. When people are called to account for these beliefs, they conjure up some or all of these considerations and construct an answer on the fly. Because these considerations are called up in a haphazard way, which manifest at any particular interview can vary, and people's assessments can be influenced by question-wording, question-order, psychological primes, and changes in their informational environment, such as what was on the news recently.

While Converse's original "non-attitudes" argument suggested that people lacked attitudes entirely, in neither the RAS model nor the cultural sociology model do authors suggest that people lack considerations. Rather, both models argue that people have internalized heterogeneous considerations that make them express differing, often strongly felt, opinions over time. Similarly, neither model prohibits people from holding strong contradictory beliefs -- support for more government spending, lower taxes, and a balanced budget, for example -- that they can present consistently over time. But these models suggest that on questions that call forth these conflicts, people will end up giving inconsistent responses as some considerations are pushed to the foreground. 

These research threads suggest that we should expect people to be inconsistent in their cultural or political beliefs when measured over time. On the morality question raised earlier, this framework expects that people will sometimes they might say they lack any opinion at all. Other times, perhaps because they recently heard a compelling argument in favor of moral absolutism, they might say they agree. Still other times, maybe after hearing a counter argument, they say might say they disagree. Regardless of what they say in the survey, they are unlikely to carry this disposition to a new setting or use it in shaping their behavior.  

## Some Do, Some Do Not

The debate about the stability of attitudes persists because at different times and in different contexts, both seem to be true. People sometimes behave as if they have clear, consistent political or cultural beliefs, and at other times appear to behave as if they do not. And this suggests a path forward.

Philip Converse's seminal work [-@converse1964; @converse1979] on attitude inconsistency is frequently invoked by other researchers to suggest that the American public lacks opinions on political matters, but his "black and white" model got its name by suggesting that on any particular issue, the population could be divided into "a 'hard core' of opinion on a given issue, which is well crystallized and perfectly stable over time" and a group of people whose responses are "statistically random" [@converse1964: p. 242]. On any particular issue, some people held opinions and others did not. The population did not consist of ideologically distinct camps, but "issue publics" that care about different issues. Converse's model has received support over time and across data sets [@converse1979; @taylor1983; @hill2001a].

Psychological research also distinguishes between strong attitudes, which are "resistant to change, stable over time, influential on cognition, and influential on action" [@howe2017: p. ] and weak attitudes [@krosnickpetty]. This research suggests that attitude strength is a multi-dimensional concept with diverse antecedents, and that people vary widely in the strength they attach to any particular attitude.

The measurement error model relies on an assumption that errors are a function of the question being asked, not the people answering questions, meaning no individual characteristics should predict instability, an assumption that runs counter to the "stong attitudes" model. While initial assessments argued that nothing predicted attitude stability [@achen1975; @erikson1979], research since finds that individual characteristics are strongly predictive of stability in political views. Zaller [-@zaller1992] shows that people who have higher political awareness -- as measured by factual information about the political space -- tend to hold more stable political attitudes. Freeder and colleagues [-@freeder2019] find that knowledge of where political parties fall on a different issue is associated with stability over time. These findings suggest, in general, some people are better at articulating consistent political opinions than others, undermining support for the measurement-error model. 

The theoretical underpinnings of Converse and Zaller's models are in no way confined to political beliefs. There's no reason to assume that a model in which some people hold stable opinions while others express ambivalence in the form of vacillating attitudes does not describe the behavior of a broad range of beliefs.  

A "some do, some do not" of attitude consistency can address theoretical problems with "latent beliefs," specifically an unclear sense of what latent beliefs are. If latent beliefs are people's theoretical preferences -- preferences for tradition over modernity; government restraint over government intervention; conservative or liberal -- that people have a hard time connecting to concrete positions or debates, then people should be more consistent in answering these theoretical positions than concrete ones. But Ansolabehere and colleagues [@ansolabehere2008] find that even when aggregated, these kinds of general attitudes are less consistent than more specific kinds of beliefs such as policy positions. A world where people have a difficult time connecting their general dispositions to specific policies but have a harder time answering questions that explicitly inquire about those general dispositions is hard to square. 

Other researchers posit that latent beliefs are "gut feelings" that take "a good deal of effort" to "put into words" [@inglehart1985: p. 101]. But, given what we know about different methods call forth different forms of culture, we should expect people to be better able to articulate their gut feeling in a forced-choice survey than in any other context, assuming a good enough answer is present in the lineup [@vaisey2009]. If latent beliefs are complicated preferences that are difficult to articulate in any setting, then they are unlikely to control behavior across domains, and we should question whether they are meaningful at all. While there are undoubtedly nondeclarative forms of culture that motivate behavior over time but would be difficult to capture in a survey [@lizardo2017], there is no reason to expect that aggregating more questions would capture these better, given their absence of symbolic content. 

A "some do, some do not" model does not need the concept of latent beliefs to explain the fact that attitudes demonstrate low constraint [@converse1964; @boutyline2017a] but are relatively stable in the aggregate [@ansolabehere2008]. Psychological research demonstrates that when people decide that an issue is important to them, they become more stable on that attitude and seek out other attitudes to reinforce it [@howe2017]. For example, a person who opposes abortion might recognize that they should also oppose government intervention in the economy because they see other people, particularly thought leaders, holding those two attitudes simultaneously [@goldberg2018]. But the extent to which they attach various attitudes, and which attitudes they attach, are going to vary from person to person, shaped by time, attention, and social networks. This will result in what Converse [-@converse1964] called "issue publics" -- socially patterned groups of people who are stable on different but overlapping sets of issues. When more attitudes get aggregated, more of these "issue publics" get folded into the average, giving the scale greater consistency as inconsistent (weak) beliefs cancel out.

A "some do, some do not" model of attitude stability, applied to attitudes more broadly, can also account for the fact that cultural attitudes can be predictive of behavior but often not as strongly as theories suggest. Researchers have invoked this weak predictive power of cultural attitudes to argue both for and against a link between attitudes and behaviors [@jerolmack2014; @vaisey2014]. A "some do, some do not model" suggests that we should not expect every person to have a position on abortion or a cultural worldview that motivates their behavior. Instead, the people who deem these beliefs to be important are more likely to have them shape their behavior, and are similarly more likely to report them stabily over time [@howe2017].

Finally, a "some do, some do not" model more closely aligns with work in cultural sociology suggesting that social structures -- institutions, contexts, and public culture -- are principally responsible for shaping attitude structure and constraint [@lizardo2010a; @martin2002; @martin2010; @rawlings2020]. Martin [-@martin2002], drawing on Durkheim's formulation of "social facts" [-@durkheim1895], argues that consistent patterns of thought require external social supports. If social reinforcement and scaffolding through public culture is a necessary condition for holding consistent beliefs, then we would not expect everybody to be equally exposed to these processes. Some people are going to be in environments where they hear contradictory messages and considerations, while other people are in environments where they hear a single line of reasoning that makes holding beliefs relatively easy [@zaller1992].

## Expectations

For too long researchers have focused on asking "do people have stable attitudes or do they lack stable attitudes?" The preceeding discussion suggests that this is a false dichotomy, rooted in a outdated understanding of attitudes. Instead, we should expect that on any particular issue that is measured in a survey, some people will hold strong, stable attitudes, while other people will demonstrate weak, ambivalent attitudes. Importantly, the distinction between these opinion behaviors might only be aparent when we look at attitudes over time, as weak attitudes make themselves apparent through their inconsistency.

There are a handful of other general expectations. First, most issues should demonstrate higher proportions of weak, vacillating attitudes than strong, stable attitudes. Questions asked in social science surveys tend to be so because because there is public disagreement on the issue, meaning most people will tend to consume heterogeneous arguments in favor and against the issue in question. Because screening out competing considerations takes cognitive work, time, and attention, which are limited resources, people are only likely to be stable on some issues while displaying ambivalence toward others.  

While there are likely issues that all people are stable on because they are taken-for-granted assumptions about the social world, for that same reason they are unlikely to be asked. Similar, there might be issues so far outside the domain of public discussion such that nobody holds strong stable beliefs, but these are also not flikely asked. 

Second, consistent with the issue publics model that roots stability in institutions and external scaffolding, people should demonstrate stability on related issues. For example, we should expect people who have stable attitudes on one question of general morality to have stable opinions on other questions of general morality, fostered through reflection on this topic, but this person might not have stable opinions on politics, religion, or even questions of specific moral prohibitions. Stable opinions on one political issue (opposition to abortion) might span to general political ideology (conservatism), but not necessarily to other political issues (opposition to government spending) [@baldassarrigelman].

Finally, we should expect very few examples of people durably changing opinions, even as we see high rates of change in the population. Previous work suggests that people tend to form dispositions early in life and carry them with them over time [@kiley2020; @vaisey2016]. As Howe and Krosnick note, "Attitudes that can be easily changed are weak and unlikely to shape behavior. The attitudes that most powerfully shape behavior are the hardest to change" [-@howe2017: p. 328]. 

# Models of Opinion Behavior

The proceeding discussion suggests that the population comprises three groups of people with distinct attitude behaviors: some will behave as if they have settled (strong) attitudes, another will behave as if they construct (weak) opinions anew in each survey wave; and a third comprises people who change their attitudes over time or ambivalent people who become strong attitude holders in the survey window. 

Most methods for measuring opinion behavior tend to conflate two of these groups or avoid quantifying their prevalence at all. Kiley and Vaisey's [-@kiley2020; @vaisey2020] methods compare the relative prevalence of vacillating changers and durable changers without quantifying the proportion that falls into these groups or the proportion giving stable responses. Focusing on how many people change their responses over time, the average level of change in responses [@freeder2019], or how many people remain stable over time, especially across only two waves, conflates people who make durable change with people who vacillate.

Other methods, especially those focused on measuring correlations over time [@alwin2007; @hout2016], treat questions as reflecting a continuous scale, rather than respecting the nominal structure of response options as they are presented to the survey participant. For example, many methods treat a change on a five point-point scale from "no opinion" to "disagree" the same as a change from "strongly agree" to "agree," even though the former represents a qualitative shift in belief while the latter represents a shift in degree. These methods also assume that a real position exists underneath measurement error [@achen1975; @ansolabehere2008], which may not be a valid assumption.

Finally, latent class methods can be used to deductively groups cases such that their manifest variables are statistically independent [@bonikowski2016]. Using such a method, we might be able to quantify how many people consistently give "agree" responses, "disagree" responses, and responses that change from one side to the other [similar to @taylor1983]. However, the theoretical model outlined above suggests more complex processes than conditional independence.

An ideal model would allow me to formally specify the three different opinion behavior groups and quantify the probability that a respondent's set of responses came from each of these theoretical models. An ideal model should respect the nominal structure of the data -- especially the unique nature of "no opinion" and other ambivalent response categories. And it should allow for a comparison with the measurement error model. 

Hill and Kreisi outline a finite mixture model congruent with the above example that they use to estimate the prevalence of these three kinds of response behavior to questions about pollution-abatement policies in a sample of Swiss residents [@hill2001a; @hill2001b; @hill2001].[^fourwave] With this finite mixture model, I can formalize three distinct theoretical processes that might have generated the observed data and estimate the probability that each one did.

## Behavioral Groups

What do these behaviors look like in practice? To clarify, consider the question from the General Social Survey's three-wave panels: "Do you agree or disagree with the following statement: Homosexual couples should have the right to marry one another." Respondents are given five options to select from: "strongly agree", "agree", "neither agree nor disagree", "disagree", "strongly disagree." Survey respondents answered this question three times over four years.

A starting assumption is that people exhibit only one of these opinion behaviors over the course of the survey window. While a person might be stable during the survey and make durable change at another point in time, in the survey window he can only exhibit one pattern. Let $\pi_j, j = 1, 2, 3$ represent the marginal probability of belonging to each group, with 1 indicating Opinion Holders, 2 indicating Vacillating Changers, and 3 indicating Durable Changers, defined below:

**Opinion Holders:** Whether because they have strong dispositions about an issue or are embedded in social structures that facilitate a single line of response over time, the first attitude behavior group, "Opinion Holders," should report being on the same side of an issue over time. With regard to the question above, a person might "strongly agree" in all three waves, or vacillate between "agree" and "strongly agree" based on recent considerations. But barring actual error in coding the response, it would be unreasonable to assume that a person who supports gay marriage would declare that they oppose it in a survey because of measurement error.[^error] Similarly, we would not expect a person who actually holds an opinion on this issue to say they "neither agree nor disagree" with the question.[^ambivalence]

[^error]: Measurement error theories are often unclear as to what "error" entails. Ansolabehere and colleagues attribute measurement error to "vague question wordings, vague response categories or categories that do not reflect the individualâ€™s actual attitude, inattentiveness on the part of the respondent, and even typographical errors" [@ansolabehere2008: 216]. While it is easy to accept that a person who supports gay marriage might vacillate between "agree" and "strongly agree" in response to this question, it is harder to imagine that a person who supports gay marriage to give a response that indicates the opposite. There will likely be cases of "measurement error" where the response is coded incorrectly, but it is hard to believe they will be so prevalent as to produce the level of vacillation that we see in some questions.

[^ambivalence]: It is debatable whether the response "neither agree nor disagree" represents ambivalence and the lack of clear opinion, as I assume here, or whether it represents a nuanced position, such as one where a person thinks that homosexuals should have the right to marry in some circumstances and not others. If it is the latter, then we would expect people to maintain this position over time. However, of the 301 people in the 2006-10 GSS panel who report this position in wave 1 and respond in each wave, only 31 maintain this position across all three waves. Results below further argue against this position.

If all people in the population behaved according to this model, we could easily model their responses over time with only two parameters. First, we would only need to know what proportion of people agreed ($\alpha_1$) or disagreed ($1-\alpha_1$) with a statment, a fixed parameter for each person, and then, in a particular wave, draw whether they give a strong ($\delta_1$) or weak response.[^strength] Theoretically, $\alpha_1$ would represent the balance of forces in society that create stable opinions in favor of an issue, such as socializing institutions.

[^strength]: The model makes the simplifying assumption that people expressing opinions are equally likely (within opinion behavior groups) to give them strongly.

**Vacillating Changers:** The second theoretical model of opinion formation suggests a very different attitude-formation process in which opinions are constructed anew in each wave on the basis of stored considerations and local influences. As noted above, several theories posit that people tend to indiscriminately consume information and construct opinions by drawing on these considerations in the survey or interview setting -- what Zaller [-@zaller1992] calls "ambivalence" [see also @hochschild1981]. These people have likely heard some arguments in favor of homosexual marriage and some arguments against it, but have not taken much time to reconcile them. In the survey interview, they are asked to give an opinion and, upon reflection, call forth some of these considerations, which might be influenced by a variety of factors, such as recent discussions. The key here is that influences on responses are local, and therefore functionally random in a large sample. This assumption reflects the idea that people in this group are somewhat removed from public debates on the issue and therefore should not be swayed by changes of elite opinion. 

While functionally different from opinion holders, these peoples' responses could also be summarized by a few parameters. We would only need to know the probability that a person gives a "no opinion" response ($\varphi_2$), the probability that, if they give an opinion it is some version of "agree" ($\alpha_2$), and the probability that if they give an opinion they give it strongly ($\delta_2$). In this model, these parameters reflect the distribution of considerations in society that people can draw on to give responses. Because their opinions are statistically independent across waves, each person's response in each wave would be a multinomial draw of the five response options ("strongly agree", "agree", "neither agree nor disagree", "disagree", "strongly disagree") defined by those three parameters.

Because responses are independent across time, any response pattern is, in theory, compatible with a model in which people generate opinions anew in each wave.

**Durable Changers:** The final group, which I call "Durable Changers," are people who truly hold opinions but go from one side of the issue to the other over the course of the survey -- saying "disagree" or "strongly disagree" in wave 1 or waves 1 and 2 but "agree" or "strongly agree" for the remainder.[^durable] People in this group might be reacting to the Democratic party's evolution on gay marriage or have a close family member of friend come out as gay, leading this person to truly change their mind on the topic. Similarly, some people might lack an opinion on the issue and say "neither agree not disagree" in the first wave, and, upon being asked to consider the question, develop an opinion for the next wave (what is commonly called "panel conditioning" [@oh2019, @halpernmanners2012]). 

[^durable]: It is plausible that in a long enough survey interval, a person might truly change their opinion twice. The surveys explored here tend to look at intervals of about four years, reducing my concern for this possibility. A change that reverts in two years does not seem very "durable," and seems to reflect more local influences than real change. 

Because this group is defined by their switching behavior, parameters to summarize them would revolve where they start in their responses and where they end, as well as when during the survey they change their attitude, which might also influence who changes their mind and how. The first parameter that describes their behavior is $\tau_3$, which captures the probability of change after the first wave. The second and third parameters, $\varphi_3^{(Pre1)}$ and $\varphi_3^{(Pre2)}$, quantify the probability that an opinion changer starts by saying they lack an opinion and changes after the first and second waves, respectively. A fourth parameter, $\alpha_3^{(post)}$ defines the probability that these people change to agreeing with the statement. 

Two more parameters, $\alpha_3^{(Pre1)}$ and $\alpha_3^{(Pre2)}$, define the probability that someone who changes from one side of the scale to the other starts by agreeing with the statement. Finally, a single parameter, $\delta_3$, quantifies the probability that if a person gives an opinion, they do so strongly. 

## Mixture Model

The fundamental challenge of the above model is that these group labels are unobserved. If we knew which group each person was in, we could just count the number of opinion holders who say they agree with a statement to get an estimate of $\alpha_1$. Conversely, if these parameters were known, we would have a simple time calculating which theoretical process was most likely to generate each response. However, while some patterns could only be generated by one of the models (e.g., "agree"-"disagree"-"agree" could only be generated by a vacillating changer), most could be generated by the vacillating changer group and one of the other groups, especially in a three-wave context.

What I can do is estimate a range of plausible values of these parameters ($\pi_1$, $\pi_2$, $\pi_3$, $\alpha_1$, ..., $\tau_3$) that are compatible with the data we observe and the assumptions I made above about these three groups. The fact that two behavioral groups exclude certain response patterns (any pattern that changes from "agree" to "disagree," or vice-versa, cannot be a stable opinion holders, and people who never change cannot be a durable changer) and some patterns are only compatible with the vacillating changer group, restrict the range of possible parameter values. The mathematical assumptions of each behavioral group, such as the assumption that each response of a vacillating changer is an draw from multinomial distribution, further restrict plausible parameter estimates. 

To do this, I use a Bayesian estimation technique known as the Data Augmentation algorithm. The algorithm has two basic steps. First, it samples the "missing data" -- group membership -- given a set of randomly drawn parameters.[^start] In other words, it asks "given this person's responses, how common each group is, and how often members of each group give certain response patterns, how likely is it that this pattern came from group 1, group 2, and group 3?" It then samples from those three probabilities and assigns a group membership to that person. Then it estimates the parameters, given group membership. It asks, "given that these people are in group 1, what's the probability of someone in group 1 saying they strongly agree or strongly disagree ($\delta_1$)?" 

[^start]: At the first stage, there are no group assignments with which to generate parameter estimates, so random values are drawn from the parameter space. These starting values have minimal, logical constraints (for example, the proportion of people in each behavioral group must sum to 1). To ensure that these starting draws do not unduly influence results, I wait for multiple chains with different starting values to converge to the same posterior distribution before using draws to summarize the distribution. 

The process iterates until it converges on a stationary distribution that summarizes the estimated parameters given the observed data, called the posterior distribution. This posterior distribution reflects plausible distributions of the parameter estimates in question, given the assumptions of the model and the observed data. Posterior distributions combine the distribution of the data given the unknown parameter values with a prior distribution of the parameters, which quantifies belief about the parameters before seeing any data. I use minimally informative priors outlined in Hill and Kreisi [-@hill2001b], the equivalent of adding two additional people to each opinion behavior group and splitting them up among behaviors within these groups.[^priors] The non-informative priors have the effect of generating slightly conservative parameter estimates in each iteration. This effect is most clearly illustrated with the durable changer group. Even in iterations where nobody in the sample is assigned to the "durable change" group, which happens quite often, the model still assumes two people display this behavior, inflating the proportion estimates away from 0. 

[^priors]: Hill and Kreisi test a range of non-informative priors -- between one and six additional people per behavioral group -- and find little sensitivity to different non-informative priors.

Over these iterations, ambiguous patterns will be alternately assigned to different behavioral groups. ... If people who cross the "opinion boundary" multiple times tend not to select strong responses, while people who stay on one side do select strong responses, this increases the probability that the latter are coming from the "opinion holder" group and not the "vacillating changer" group, even if they could be generated by both.






# The Basic Probability Model



## Estimation






Finite mixture models allow researchers to specify separate groups with different data-generating processes governed by different parameters, and then estimate the probability a particular observation came from that group [@hill2001b; @]. Applied to the question at hand, I can specify different data-generating processes for each behavioral group, with different parameters and quantify the probability that each process generated the observed data. The fundamental challenge is that membership in each group is unknown, and many responses could reflect two of these groups. As a result, group membership has to be inferred from the respondent's behavior in the panel study and the overall distribution of response patterns. A person who says "agree"-"disagree"-"disagree" has some probability of coming from the "durable change" group and some probability of coming from the "vacillating change" group, probabilities that can be determined by the behavior of the overall sample.




The parameters in this model give us an expectation for how frequently response patterns that *look* like stable opinions or durable change are actually reflective of vacillating change, given the distribution of patterns by assuming that the probability that a vacillating changer reports particular responses is consistent over time. That means for every vacillating changer who says "agree-disagree-agree" there should be one who says "disagree-agree-agree" and one who says "agree-agree-disagree." While we could not be certain which of these latter two patterns were vacillating changers and which were durable changers, the overall prevalence of these behaviors in the population, relative to the first pattern, can give us an estimate of the proportion of each group in the population. 


## Estimation

Rather than estimate the model directly on people's response patterns, which is computationally impossible with a finite sample, I re-express the data as a series of general variables that describe respondents' behavior, such as the number of times they cross the "opinion boundary" and the number of "strong" responses they give. Full details on this recoding and how the model is estimated are outlined in Appendix A. 

To estimate the parameters I use a Bayesian estimation procedure known as the Data Augmentation (DA) algorithm. 




A challenge in using panel data is that people drop out of the sample or refuse to answer questions, leaving missing data. In the analyses presented here, I assume that data is Missing at Random, meaning that once I condition on a person's observed responses, response rates are random. Hill [-@hill2001] finds that the distribution of parameter estimates is similar under a variety of assumptions, with the proportion of vacillating changers slightly higher under less restrictive assumptions than the one made here. Accommodating this assumption adds an additional step to each iteration: filling in missing data on the basis of group assignment and the parameter estimates. For example, if a person is assigned to the "stable opinion" group and expresses either agree or strongly agree in their non-missing waves, their missing response is filled in from a draw of "strongly agree" and "agree" with probabilities $\delta_1$ and $1-\delta_1$. 

The model iterates through these three steps -- assign group membership on the basis of responses and estimated parameters; estimate parameters on the basis of group membership and responses; and fill in missing data on the basis of parameters, group membership, and observed data -- until iterations reach a stable posterior distribution.

## Gay Marriage Example

To demonstrate the model and the interpretation of its parameters, I apply it to the gay marriage question described above. This question has several features that make it a good illustration. Its structure is similar to the questions that Hill and Kreisi developed the model with: a five-point Likert scale with a midpoint that can be thought of as "no opinion" or ambivalence. Prior to and during this period, gay marriage attained a high profile in national political debates, which should ensure that some proportion of the sample had clear opinions. There were also changes in elite opinion on the issue during the window studied. As a result, I expect that some members of the sample will demonstrate durable change.

Figure \@ref(fig:gaymarriage) summarizes the posterior distribution of the gay marriage example, with the mean of the posterior distribution for each parameter indicated by a circle and error bars indicating a 95 percent confidence interval. 

[Figure \@ref(fig:gaymarriage) about here.]

```{r gaymarriage, fig.cap='Parameter estimates of Fininte Mixture Model for Gay Marriage question in 2006-14 GSS panels. Posterior distribution estimated with 5 chains, 2500 iterations each, first 500 iterations of each chain discarded. "N.O." inficates "No opinion/Neither agree not disagree" response. "Chg." indicates "change".', fig.align='center', fig.height=6}
load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/marhomo_model.Rdata")

marhomo_model$pattern_param_summary %>%
    filter(param %!in% c("llike", "chain", "sd_g1", "sd_g2", "sd_g3")) %>%
    mutate(grp = ifelse(param %in% c("pi1", "pi2", "pi3"), "Behavioral Group",
                        ifelse(param %in% c("alpha1", "delta1", "gamma1"), "G1: Opinion Holder",
                               ifelse(param %in% c("alpha2", "delta2", "phi2", "gamma2"), "G2: Vacillating Changers",
                                      "G3: Durable Changers")))) %>%
  mutate(param = recode(param, "pi1"="Pr(Opinion Holder)", "pi2"="Pr(Vacillating Changer)", 
                        "pi3"="Pr(Durable Changer)", "alpha1"="Pr(Agree)", "delta1"="Pr(Strong|Opinion)",
                        "phi2"="Pr(N.O.)", "alpha2"="Pr(Agree)", "delta2"="Pr(Strong|Opinion)",
                        "tau3"="Pr(Chg. w1-w2)", "delta3"="Pr(Strong|Opinion)",
                        "phi3pre1"="Pr(w1=N.O., Chg. w1-w2)", "phi3pre2"="Pr(w1=N.O., Chg. w2-w3)",
                        "alpha3pre1"="Pr(w1=Agree, Chg. w1-w2)", "alpha3pre2"="Pr(w1=Agree, Chg. w2-w3)",
                        "alpha3post"="Pr(Agree|w1=N.O.)")) %>%
    ggplot(aes(x = param, y = mean)) + 
  geom_hline(yintercept = c(0,1), color = "black") +
    geom_linerange(aes(ymin = q25, ymax = q975)) + 
    geom_point(shape = 21, fill = "gray") + 
    coord_flip() + 
    theme_bw() + 
    expand_limits(y=c(0,1)) +
    facet_grid(grp ~ ., scales = "free_y") +
    labs(x="", y = "Coefficient estimates (95%CI)") +
  theme(strip.text = element_text(size = 8),
        axis.text=element_text(size=8)) 

```

As a top-line result, the model estimates that between 49 and 53 percent of the sample gave stable responses to this question over three years, either agreeing ($\alpha_1 = .40$) or disagreeing with the question in all waves, while between 42 and 47 percent of the sample gave vacillating responses. Between 3.28 and 6.45 percent of the sample made durable changes in attitudes during the survey window (2006-14). 

Is about half the population holding a stable view on the gay marriage issue a surprising number? It is significantly higher than the 20 percent of people Converse suggested had stable opinions on the issue of government control of utilities [@converse1964], and it is roughly comparable to what Hill and Kreisi found when they explored opinions on pollution-abatement policies [@hill2001a]. That being said, it was a dominant political issue of the time, and the fact that more than four in ten people lacked durable opinions on the issue is notable.

Other estimates in Figure \@ref(fig:gaymarriage) are consistent with our intuitions about attitude change and stability. People who hold stable attitudes are more likely to report "strong" attitudes than people who hold vacillating attitudes [@howe2017]. Nothing in the model requires this; it is simply a function of response patterns. Vacillating changers said "neither agree nor disagree" about a quarter of the time, and when they gave an opinion they were about evenly split between agree and disagree ($\alpha_2 = .44$). 

Consistent with the historical record, the group of people who made durable change tended to change from opposing gay marriage to supporting it ($\alpha_3^{(pre1)} = 0.318$ and $\alpha_3^{(pre2)} = 0.081$). They were also more likely to change between the first and second wave ($\tau_3 = .68$) than between the second and third wave, consistent with theories of panel updating.[^years] Durable changers more closely resemble opinion holders than vacillating changers in their extremity, though there is more uncertainty in parameter estimates for the durable change group, since the group is smaller. 

[^years]: The full panel collapses three three-wave panels, so wave 1 actually represents three two-year periods (2006-08, 2008-10, and 2010-12). This makes it difficult to draw conclusions about timing of change, though analyzing the panels separately can provide clarification.

To this point, the model has done nothing to rule out the measurement error model, which suggests that the population comprises one group with different latent positions, responding with errors that are normally distributed. The model presented above could just be partitioning people based on their position in the scale. If this were the case, however, we would expect people in each group to demonstrate similar levels of within-person change over time. People in the stable opinion group would have a latent position near the ends of the spectrum (bouncing between 1 and 2 or 4 and 5), while vacillating changers and durable changers would have latent positions in the middle of the distribution (bouncing between 2 and 3 or 3 and 4), and all groups would respond with similar average errors. While people with extremely large variation would have to be in the vacillating changer or durable changer group, they should be relatively rare so that, on average, the groups are more or less comparable. 

To compare the three-group model to the measurement error model, Hill and Kreisi [-@hill2001a] estimate the average within-person standard deviation for each behavioral groups at each iteration. Table 1 shows the average within-person standard deviation for the three groups. 

| Behavioral Group     | $\sigma$ (95% C.I.)  |
|:---	                 |:---:                 |
| Opinion Holder       | 0.303 (0.295, 0.312) |
| Vacillating Changer  | 0.909 (0.885, 0.932) |
| Durable Changer      | 1.469 (1.308, 1.631) |
Table 1: Average within-group standard deviation, by opinion behavior group.

```{r, include = FALSE}
marhomo_model$pattern_param_summary %>%
  filter(param %in% c("sd_g1", "sd_g2", "sd_g3"))
```

Table 1 shows that the average within-person standard deviations are different across groups. Opinion holders have a small average standard deviation ($sd_{G1} = .303$). Vacillating changers have average within-person standard deviations about three times as large as opinion holders ($sd_{G1} = .909$), and durable changers have an average standard deviation larger still ($sd_{G1} = 1.469$). 

In other words, vacillating changers and opinion holders do not simply reflect similar behavior at different points in the scale. Vacillating changers demonstrate much more wave-to-wave change than opinion holders, choosing strong responses more often than we would anticipate if they had similar error ranges to opinion holders but had an average near the scale midpoint. Durable changers appear to swing from one end of the scale to the other, rather than give responses near the middle of the scale. As Hill and Kreisi note in their examination of the Swiss pollution-abatement data, "Given the implausibility of assuming vastly different measurement errors for different people, it seems rather more likely that this variation is composed of both measurement error and true opinion instability" [-@hill2001a: p. 408]. 

# Full Data

The above example demonstrates the model's ability to separate out three distinct sets of opinion behavior, and it provides a baseline for what we might expect for rates of stability, vacillation, and durable change. I now turn toward examining a wider range of attitudes across several panel data sets.

A key challenge in the study of attitudes and opinions is that while it is possible to take a random sample of respondents to generalize to a population of people, it is unclear how to randomly sample to draw inferences to a larger population of attitudes. In the absence of a random sample, many studies explore one or a few attitudes and assume that other attitudes behave similarly. I cast a wide net to capture as broad a range of attitudes and beliefs as possible. It is important to keep in mind that these attitudes do not represent a random sample, so statistical tools to draw inferences from this sample will likely be misleading.

The method outlined above requires at least three waves of responses by the same person to the same question. I focus on questions that ask respondents to give a statement of opinion or judgment on an issue. These questions are often structured as five-point Likert scales, which is the structure Hill and Kreisi designed the model to address. However, to apply this approach to a wider range of questions I make a handful of modifications to the model and the questions being examined.

Many questions lack a clear "no opinion" midpoint. However, interviewers tend to record a volunteered "no response" option. Assuming in the absence of a "no opinion" option people to select a second-best option or volunteer "no opinion," the effect would be a decreased value of the $\varphi$ parameter, but it should not affect the distribution of the behavioral group parameters. Some questions with midpoints are not labeled "no opinion", but rather some other phrase. In cases where the language or substantive interpretation of this midpoint option is equivalent to "no opinion" or ambivalence, I employ the model as it is outlined above. In cases where the midpoint has a substantive interpretation other than "no opinion," I collapse the response into a binary response structure, which I discuss below.

To accommodate two- or three-point scales -- "agree"/"disagree" or "yes/no," often with a "no opinion" or "neither agree nor disagree" option -- I remove the $\delta$ parameters from the model. The lack of "strong" response options has the consequence of greater uncertainty about whether a particular response pattern falls into one behavioral group or another, and the within-group standard deviation comparison is not possible with these questions as opinion holders will always have no within-person standard deviation. 

Similarly, I adapt the model to handle seven-point scales by adding an additional parameter ($\gamma_j$) to account for "weakly" agree and "weakly" disagree or comparable points (response 3 and response 5 on a seven-point scale). For most questions, I maintain the assumption that the midpoint of these scales reflects a lack of opinion rather than a substantive "moderate" position. This might be a better assumption for some questions than others. However, if people hold substantive "moderate" positions, it will show up as lower average within-person variance for these questions.[^moderate] Some questions have much larger scale ranges, including 10-, 12-, or 100-point feeling thermometers. When these questions have symmetrical structures, I collapse them questions into five-point scales.[^thermometers] 

[^moderate]: Kinder and Kalmoe [-@kinder2017] suggest that on seven-point political ideology scales, moderates are "indistinguishable from those who remove themselves from ideological analysis" (p. 160) and that we should regard these people as people who lack opinions, not people who hold sophisticated positions in the middle of the distribution. This strengthens my confidence on using the midpoint as a non-attitude point, rather than a moderation position.

[^thermometers]: On 100-point scales, respondents less than or equal to 20 and 80 as giving considered to give "strong" responses, and individuals at 50 are considered to give ambivalent responses.

Finally, I modify some questions to conform to the model by changing non-symmetrical scales to a binary structures. For example, the General Social Survey includes a question about people's interpretation of the Bible: "Which of these statements comes closest to describing your feelings about the Bible? a. The Bible is the actual word of God and is to be taken literally, word for word. b. The Bible is the inspired word of God but not everything in it should be taken literally, word for word. c. The Bible is an ancient book of fables, legends, history, and moral precepts recorded by men." I collapse this into whether a person chooses Biblical literalism (choice a) or not. In this way, it conforms to the binary model (without a delta parameter in any group). This approach sacrifices nuance (people who believe the Bible is inspired by God and those who believe it is a book of fables are considered the same, and changes between these beliefs do not count for anything) for direct comparability with other questions. In these cases, I try to focus on the most substantive division in the structure. 

A search of multiple panel data sets produced 544 questions for analysis. For this analysis I use data from the General Social Survey's rotating three-wave panel (166 questions)[^gsspanel]; the American National Election Studies panels from 1956-60 (10 questions), 1972-1976 (71 questions), 1992-1997 (66 questions), and 2000-2004 (57 questions); and the Cooperative Congressional Election Study (36 questions). I also use data from the National Study of Youth and Religion (45 questions), two panels -- one of mothers, one of their children -- from the Intergenerational Study of Parents and Children (17 questions in the mother panel, 18 in the child panel), and the Longitudinal Study of Generations (58 questions). These data sources have distinct advantages when compared with each other. The GSS, ANES, and CCES panels are representative samples of U.S. adults over the age of 18. The NSYR is a nationally representative sample of young people, who are not included in the GSS panels, and includes questions about religious and moral issues that the adult panels lack. The LSG and ISPC are not a probability samples, so it is hard to make inferences about the larger population from these samples, but they include distinct sets of attitude items and allow for the analysis of some questions in different temporal or generational settings.

[^gsspanel]: The GSS panels were collected over an eight year period as a series of three rotating three-wave panels. Each panel extended over a four-year period (2006-2008-2010; 2008-2010-2012; 2010-2012-2014). Because of the significant overlap in time periods, I treat the GSS panels as a single panel.

The panels also cover different time intervals. The GSS, ANES, and CCES panels cover about four years. The ISPC panel covers 13 years. Other panels fall between these. In general, I suspect that stable opinions will be easier to hold over shorter time frames, so panels covering a longer time frame will see more vacillation and durable change.

It is impossible to say whether the attitudes captured in the 544 questions analyzed here are "representative" of attitudes in general. Because of the nature of the surveys and the kinds of topics get covered in panel studies that extend at least three waves, political views make up a large proportion of questions in this analysis. However, the data sets -- particularly the GSS -- contain a broad array of questions on other topics including beliefs about race relations, gender roles, family structures, moral worldviews, intergroup relations, and social and institutional trust.

Appendix B lists the full range of questions analyzed here, as well as any modifications to the question structure. 

# Results

I apply the finite mixture model outlined above to these 544 questions. My discussion of the results proceeds in three sections. I first examine the overall distribution of the three response patterns across questions, drawing general conclusions about attitude stability and change across data sets and questions. Next, I use a multiple regression approach to evaluate the effects of question content and structure on rates of stability and change. Finally, using results from the 166 questions from the GSS, I examine the pairwise correlations of stability estimates to look for evidence of "issue publics," or sets of issues where stability tends to co-occur. I focus on the GSS because it includes the most questions touching on the widest variety of issues.

## Overall Pattern Proportions

As a first step in this analysis, Figure \@ref(fig:patterncomp) presents a series of boxplots, three for each data set, plotting the distribution of mean estimates for the proportion of durable changers, vacillating changers, and opinion holders for each question. The figure contains a lot of information, but it highlights some clear patterns across the data sources.

[Figure \@ref(fig:patterncomp) about here.]

```{r patterncomp, fig.cap='Mean proportion of respondents in each behavioral group, by question and data set. Posterior distribution estimated with 5 chains, 2500 iterations each, first 500 iterations of each chain discarded.', fig.align='center'}
results_df %>% 
  filter(param %in% c("pi1", "pi2", "pi3")) %>% 
  mutate(param = recode(param, "pi1"="Stable Opinions", "pi2"="Vacillating Changers", 
                        "pi3"="Durable Changers")) %>%
  mutate(ds = recode(ds, "gss"="GSS (2006-2014)", "saf-m"="ISPC-Mom (1980-1993)",
                     "saf-c"="ISPC-Child (1980-1993)", "lsg"="LSG (varies)", 
                     "nsyr"="NSYR (varies)", "anes7276"="ANES (1972-1976)",
                     "anes5660"="ANES (1956-1960)", "anes9297"="ANES (1992-1996)",
                     "anes0004"="ANES (2000-2004)", "cces"="CCES (2010-2014)")) %>%
  ggplot(aes(x = ds, y = mean, fill = ds)) + 
  geom_hline(yintercept = .5, linetype = 2, color = "gray") +
  geom_quasirandom(shape = 21, alpha = .3) + 
  geom_boxplot(alpha = .1) + 
  coord_flip() + 
  expand_limits(y = c(0,1)) + 
  theme_bw() + 
  theme(legend.position = "none") + 
  facet_wrap(~param) + 
  labs(x = "", y = "Proportion of Respondents")
```

Starting at the left, the figure shows that durable change is rare across all data sets and almost all questions. For about half of all questions, the mean estimated proportion of respondents demonstrating durable change is less than 5 percent, with many of these questions showing rates statistically indistinguishable from 0.[^conservative] Only 17 percent of questions have mean rates of durable change above 10 percent. Attitudes that exhibit very high levels of attitude change typically pertain to major public events, and are exceptions that prove the rule that durable change is, in general, rare. 

[^conservative]: Because of the conservative nature of the estimation process, the estimate for the proportion of durable changers can never be 0. But most of these questions have confidence intervals that extend below 1 percent. 

Second, there is a range of ambivalence and stability across the data sets. Most of the data sets analyzed here include questions that demonstrate both high (greater than 70 percent) and low (less than 30 percent) levels of stability and vacillation. Since within-survey comparisons reflect the same sample, vacillation is not a feature of the population (the kinds of people being surveyed) but an interaction between people taking the survey and the kind of question being asked. In other words, there are some questions that most of the population can answer consistently over time, and there are questions where very few people in the population can answer consistently over time. However, most questions fall somewhere in the middle, with some people answering consistently and others not. 

Third, vacillating change is on average more common than stability for the questions analyzed here. For 56 percent of the questions, more than half the sample demonstrated vacillating change. The 1956-60 ANES panel, which Converse analyzed to generate his original insights about non-attitudes, displays the highest average level of vacillation, but it is not an outlier. Almost all other surveys include questions that surpass that average in ambivalence. The GSS and the CCES are the only data sets where more questions show higher levels of stability than vacillation.

There are other features of the results that should give us confidence that the finite mixture model approach separates distinct sets of opinion behavior and is not just partitioning respondents based on their positions on the scale. Figure \@ref(fig:strongcomp) plots the proportion of opinion holders and vacillating changes who give strong responses, conditional on them expressing an opinion (not saying "no opinion" or "neither agree nor disagree"), for questions with four, five, or seven response options. Because they lack "strong" responses, questions with two and three response options are excluded here. Points above the line suggest that people in the opinion holder group give "strong" responses at a higher rate than vacillating changers, conditional on giving an opinion.

[Figure \@ref(fig:strongcomp) about here.]

```{r strongcomp, fig.cap='Mean estimated proportion of "strong" responses in opinion holder and vacillating changer behavioral group. Posterior distribution estimated with 5 chains, 2500 iterations each, first 500 iterations of each chain discarded.', fig.align='center', fig.height=4, fig.width = 4}
results_df %>%
  left_join(qm, by = c("var"="var_name")) %>%
  filter(param %in% c("delta1", "delta2")) %>%
  select(param, mean, var, toptions, ds.x) %>%
  spread(param, mean) %>%
  ggplot(aes(x = delta2, y = delta1)) + 
  geom_point(shape = 21, fill = "gray") + 
  geom_abline(slope = 1) +
  expand_limits(y = c(0, 1), x = c(0, 1)) + theme_minimal() + 
  labs(x = "Pr(strong opinion | agree or disagree, vacillating changer)",
       y = "Pr(strong opinion | agree or disagree, opinion holder)") 
```

For almost all questions with a "strongly agree" option, people with stable attitudes are more likely than people with vacillating attitudes to respond with that position, often much more likely. This suggests vacillating changers are generally expressing weaker opinions than opinion holders. At the same time, respondents who vacillate still select the "strong" option, often quite frequently suggests that they tend to have a larger response range than opinion holders. 

If the measurement error model is correct, within-person standard deviations should be comparable across behavior groups. If standard deviations are not comparable, we have more confidence that these groups exhibit different opinion behaviors. Figure \@ref(fig:sdcomp) presents two sets of plots. The top row compares vacillating changers to durable changers, and the bottom row compared vacillating changers to opinion holders. Because questions with 2 or 3 response options will always have a within-person standard deviation of 0 for the opinion holders group, they are omitted from this comparison. Points above the line indicate questions where the vacillating changers have greater within-person variance than opinion holders.

[Figure \@ref(fig:sdcomp) about here.]

```{r sdcomp, fig.cap='Average within-person standard deviation, by behavioral group and question response options. Solid black line indicates equal within-group standard deviations. Dashed line indicates largest average within-person standard deviation for stable opinion holders. Posterior distribution estimated with 5 chains, 2500 iterations each, first 500 iterations of each chain discarded.', fig.align='center'}

comp12 <- results_df %>%
  left_join(qm, by = c("var"="var_name")) %>%
  filter(param %in% c("sd_g1", "sd_g2")) %>%
  filter(toptions > 3) %>%
  mutate(toptions = paste("Answer choices = ", toptions, sep = "")) %>%
  select(param, mean, var, toptions, ds.x) %>%
  spread(param, mean) %>%
  mutate(out = sd_g1) %>%
  mutate(outcome = "Opinion holder avg. s.d.") %>% select(-sd_g1)

comp23 <- results_df %>%
  left_join(qm, by = c("var"="var_name")) %>%
  filter(param %in% c("sd_g2", "sd_g3")) %>%
  filter(toptions > 3) %>%
    mutate(toptions = paste("Answer choices = ", toptions, sep = "")) %>%
  select(param, mean, var, toptions, ds.x) %>%
  spread(param, mean) %>%
  mutate(out = sd_g3) %>%
  mutate(outcome = "Durable changer avg. s.d.") %>% select(-sd_g3)

lines <- data.frame(toptions = rep(c("Answer choices = 4", "Answer choices = 5",
                                "Answer choices = 7"), 2),
                    outcome = rep(c("Opinion holder avg. s.d.", "Durable changer avg. s.d."), each = 3),
                    line = rep(c(0.422, 0.422, 0.682), 2)) 

bind_rows(comp12, comp23) %>%
  ggplot(aes(x = sd_g2, y = out)) + 
  geom_vline(data = lines, aes(xintercept = line), linetype = 2) + 
  geom_hline(data = lines, aes(yintercept = line), linetype = 2) + 
  geom_point(shape = 21, fill = "gray") + 
  geom_abline(slope = 1) + 
  expand_limits(y = 0) + 
  facet_grid(outcome~toptions, switch = "y", scales = "free_x") +
  theme_bw() + 
  labs(x = "Vacillating Changers: Average within-person S.D",
       y = "") + 
  theme(strip.placement = "outside",
        strip.background = element_blank())
```

Consistent with the gay marriage example presented above, Figure \@ref(fig:sdcomp) shows that vacillating changers uniformly have higher within-person standard deviations for almost all questions with four or more response options, regardless of question structure, and almost uniformly lower within-person standard deviations than durable changers.[^not_durable] This suggests that durable changers are more likely to move from extreme ends of the scale, while vacillating changers are more likely to cluster in the middle of the scale (though with more variation than opinion holders). In other words, we can be fairly confident that the model is separating three different groups with distinct sets of behavior. Notable here is the fact that the pattern is consistent with most seven-point questions. On five-point questions, vacillating changers might have more room to move around and express their nuanced opinions (three positions: agree, neither agree nor disagree, and disagree) than opinion holders (strongly agree and agree). But even when opinion holders have room to give a wider range of responses, they still demonstrate uniformly less within-person variation than vacillating changers.

[^not_durable]: The three questions where durable changes have lower average within-person standard deviation than vacillating changers ask respondents if if the government should bus students to ensure racial equality (72-76 ANES), whether the government should do more to improve the conditions of blacks (92-96 ANES), and whether they are happy with their bodies (NSYR).

As an overall conclusion, we can say that the model is detecting three distinct sets of behavior, consistent with Converse's original black and white model, with the addition of a durable change group. Opinion holders tend to exhibit strong opinions at the ends of scales. Vacillating changers, rather than reflecting relatively settled views at scale midpoint, tend have a much wider range of responses that vacillate on either side of the issue. Durable changers tend to move between ends of the scale, often from one extreme to another. While we can say that overall, durable change is rare, there is a large variance in stability and vacillating change that remains to be explained. 

Give the large number of questions examined and the covariation in content and structure, it is difficult just looking at the overall patterns to say whether it is question content or structure that drives overall rates of stability or vacillation. The GSS has more questions with fewer response options (questions with two, three, or four choices make up most of the GSS questions), but also more questions about non-political content. The ANES panels tends to favor questions with more response options, typically five or seven, but focuses almost exclusively on questions of politics. An additional feature of question structure that might matter is the explicit option of a midpoint to signal ambivalence. Appendix C outlines a regression approach to evaluate the competing influences of structure and content on attitude stability and change. 

Overall, structure and content about 50 pecent of the variation in rates of vacillation. Questions with fewer response options demonstrate higher stability and less vacillation. In terms of content, questions about sexuality and religion demonstrate slightly higher rates of stability than questions about politics, and questions about general morality and civil liberties show higher rates of vacillation. Questions about race, foreign relations, work, crime, and family and gender are not more or less stable than general political questions once I control for question structure. 

There are some questions that are poorly explained by these tags. Across data sets, partisan identification is more stable than other measures of general political ideology (position on liberal-conservative scale; role of government in economy; role of government in equal opportunity, etc.). Another pattern in the residuals is that questions about specifics -- specific policies and specific behaviors -- tend to be more stable than predicted while questions about general concepts (ideology, broad morality) tend to vacillate more than predicted. This helps explain why the CCES, which includes many questions about specific policies, has a higher rate of stability than other political surveys.

Content explains less of the variation in rates of durable change, and there is no clear pattern in terms of question structure. However, the NSYR and the child panel of the ISPC, the two panels with with youngest average age, show higher rates of durable change than other panels, even controlling for questions strucutre and content. The largest residual in the durable change model pertains to people's evaluation of Richard Nixon in the 1972-76 ANES panel, in which a large proportion of people shifted from a positive opinion or no opinion to a negative opinion. Given that this window covers from Nixon's victory in the 1972 election through the Watergate scandal and his eventual resignation, this shift is not surprising.

## Issue Publics

The preceding results suggest that, on any particular issue, some people seem to hold opinions while other people seem lack them. While this is strongly suggestive of an issue publics model, it does not rule out the probability that what drives stability resides at the individual level, and that questions simply vary in the degree to which people can stably answer them. To test whether these results are consistent with the "issue publics" theory, I explore the correlation of the pairwise probabilities of giving stable responses to questions, generated by the proportion of times that a respondent is classified in each group over the 10,000 iterations of the model. If stability is socially patterned, then people should be stable on related questions but not necessarily stable on unrelated questions. If a measurement error model is correct, then the probability of being stable on one question should not be related to their stability on any other questions. If stability is principally a function of individual cognitive ability or other individual feature, then pairwise correlations should be strong, as people who hold stable opinions do so across the board and other people fail to.

The 166 questions in the General Social Survey panels examined here produce 13,695 pairwise correlations.[^consolidate] In general, pairwise correlations are very low, with an average correlation of .04, suggesting that stability in one attitude is not a strong predictor of stability in another attitude. This provides additional support for the argument that attitude stability is not principally a function of the person answering the question. At the same time, some pairwise correlations are quite strong. To better understand the distribution of these strong correlations, I plot them as a network diagram in Figure \ref{fig:stabgraph}. For parsimony, I focus on the 541 correlations greater than $\rho > .2$, about 2 percent of all correlations. Ties between issues indicate that people who are stable on one issue are stable on the other, and that people who vacillate on one issue vacillate on the other. 

[^consolidate]: There are seven federal spending questions where different versions are tested for different people, examined separately above. The two versions show minimal differences, so I consolidate each pair into a single item for this analysis.

[Figure \@ref(fig:stabgraph) about here.]

```{r stabgraph, fig.cap='Network graph of pairwise correlations of co-occuring stability greater than .2. Probabilities generated from group assignments over 10,000 draws from posterior distribution.', fig.align='center', fig.height=9.5, fig.width = 6.5}
load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/stabcors.Rdata")
stab.cors[abs(stab.cors) < .2] <- 0
stab.cors[abs(stab.cors) > .2] <- 1
sc2 <- stab.cors[rowSums(stab.cors, na.rm = TRUE) > 1, colSums(stab.cors, na.rm = TRUE) > 1]
stab.g <- graph_from_adjacency_matrix(sc2, mode = "undirected", 
                                      diag = FALSE)
par(mar=c(0,0,0,0)+.1)
plot(stab.g, 
     vertex.size = 11,
     vertex.color = adjustcolor("gray", alpha.f = .5),
     vertex.label.cex = .5,
     vertex.label.color = "black",
     layout=layout.fruchterman.reingold(stab.g))

```

Figure \ref{fig:stabgraph} reinforces the "issue public" nature of attitude stability, showing a series of spanning trees, with the occasional sense cluster of attitudes. It is important to keep in mind that the figure does not reflect the correlation of the underlying issues themselves, but rather the correlations of the probability of stability. First, 58 questions in the GSS do not demonstrate any substantial correlations with any other questions. This group includes a heterogeneous mix of questions about gun laws, whether there is an afterlife, whether premarital sex is acceptable, and more. However, closely related questions tend to demonstrate simultaneous stability. The largest component of the network is centered on a dense cluster of questions about civil liberties, but it spans out to questions of police use of force, abortion, and the values that should be demonstrated by children. At the same time, this component is totally disconnected from questions about political views. 

Also notable in the figure are the lack of connections between certain issues. Despite having a similar structure and appearing next to each other in the survey, sets of questions about abortion are completely disconnected in the figure -- holding a stable belief about abortion in some circumstances (child defect, women's health, and rape) does not necessarily beget a stable position on other circumstances (single mother, as a method of birth control, if the mother is poor, and any circumstance the woman wants). The same is true of beliefs about suicide. Similarly, stability in general political beliefs (liberal-conservative ideology, partisan identification) is completely disconnected from a broad set of questions about government spending. Questions about general social trust are disconnected from specific questions about confidence in institutions, and questions about specific morality are disconnected from questions about general morality.

## General Discussion of Results

The results presented above suggest several general conclusions about the behavior of attitudes, at least the attitudes that tend to get measured in social science surveys. First, it is wrong to say that people, in general, do not hold durable opinions. On most issues, there is some proportion of the population that consistently locates itself on one side of an issue or another. Consistent with psychological research on opinions, when people hold opinions stably, they are disproportionately likely to say they hold this opinion strongly [@howe2017]. While simpler questions -- with fewer answer choices or less abstract wording -- seem easier for people to answer consistently over time, some proportion maintain stable attitudes even on complicated, abstract, and conceptual questions.

At the same time, it is also wrong to say that people in general "have" opinions, as in common in latent attitude and measurement-error models. On any particular question there is typically a large group of people who do not maintain consistent opinions, vacillating between ends of the scale in ways that suggest they are subject to short-term considerations. Examination of the average within-person standard devisions for different groups suggest that people who vacillate are not just people at the middle of the scale who report with error; they vacillate more widely than people who hold stable opinions. These patterns are strongly consistent with Zaller's notion of ambivalence, that on any particular issue, "people are likely to internalize many contradictory arguments" and "form considerations that induce them both to favor and oppose the same issues" [-@zaller1992: p. 59]. This is true not just of politics, but general morality, religious beliefs, and more.

A striking pattern in the results presented here is similarity of attitudes across topic domains. It is not the case that different kinds of attitudes behave differently. While there was a large range in the amount of stable opinions and vacillating change across questions, there were no questions analyzed here where all people demonstrated stability and no questions where all people demonstrated vacillating attitudes.[^selection] This suggests that the general mechanisms that produce attitude stability or vacillation are present across domains, but the specifics might vary widely. Similarly, most questions demonstrate low levels of durable change, with the exceptions reflecting high-profile events that are, by necessity, rare. It is not the case that people are forming and reforming attitudes that they carry with them over medium time frames. 

[^selection]: It is plausible that such questions exist but make for bad survey questions. Survey questions tend to focus on areas where there are disagreement in the public. 

In contrast to expectations, people are not inherently better at answering non-political questions than political questions, with the exception of questions about religious beliefs and sex. While questions about political issues do demonstrate high levels of ambivalence, they are not unique in that regard. People are similarly ambivalent about morality, civil liberties, race, gender, family structures, medicine, and more. In fact, when issues in other domains -- race, social trust, medicine, etc. -- interact with the political space, people seem more likely to report the same opinion consistently. This suggests, consistent with Zaller's model, that people get their cues about what to believe from opinion leaders, regardless of domain. 

Across domains, specific attitudes tend to be more stable than general attitudes. People are much more likely to be consistent on specific moral prohibitions than on general questions about morality (whether morality is a personal matter, whether issues are black and white or contain shades of gray, whether morality should change with the times). They are more stable on specific questions about government spending on various priorities than on general questions about the role of government and political ideology. This suggests that people work inductively from specifics to general principles, and that they seem to have a hard time reconciling disparate specifics. They do not seem to apply general principles to specific outcomes. In the political domain, this is consistent with the notion that people are "ideologically innocent" [@kinder2017], and further argues against general or latent beliefs. This, combined with the fact that issues demonstrate greater stability when they intersect with the political realm, could also suggest that stability on issues is a function of what people hear, with specific issues more frequently discussed than general principles. 

## Conclusions

This paper sought to adjudicate a longstanding divide present in both cultural sociology and public opinion scholarship between theories suggesting that people lacked clear opinions and those suggesting that people held stable, real opinions. Going back to Converse's original formulation of the "black and white" model, I suggest that for any particular question, both were likely true: some people hold real, stable attitudes while other people lack stable attitudes altogether. Using a finite mixture model approach, I found that all questions could be divided into people who hold stable opinions, people who vacillate, and a small group of people who form an opinion or change their opinion during the course of the survey, with the vacillating group tending to outnumber opinion holders. This set of results is strongly consistent with Zaller's model of ambivalence, but also suggests that some people in the population do form stable, real opinions. 

For cultural sociology, the results presented here reinforce notions that opinion stability is a function of social scaffolding -- institutions, context, and social networks. The large variation in stability across questions and the clustering of stability on related topics, suggest that features of the social environment facilitate attitude stability. Exactly what kinds of social scaffolding, and where in the life course their influence matters most, is unclear from these results. It could be the case that people with stable dispositions acquire these early in life, while people who do not acquire these early never do. It could also be the case that people who hold stable attitudes do so because they are embedded in social structures that facilitate them. Adjudicating the mechanisms that facilitate stability requires further work.

The results have broad implications for theories of social change. Kiley and Vaisey [-@kiley2020; @vaisey2020; @vaisey2016] posited that people have relatively stable views by the time they reach adulthood, especially for the GSS questions they analyzed. This is reinforced here by the low proportions of people demonstrating durable change. The NSYR demonstrates the highest rate of durable change. Similarly, the child panel of the ISPC demonstrates much higher rates of durable change than the mother panel, and both panels contain the same questions, extend for the same duration, and are asked in the same years. 

At the same time, their conclusion that people have settled dispositions is challenged by the high amount of vacillation in many questions. While we can say that large swaths of the population have settled dispositions about questions about sexual morality, religious beliefs, and some specific governmental policies, they lack consistent opinions on more general conceptual questions -- political ideology, general beliefs about the role of government, general morality, and abstractions about civil liberties. If people tend to be ambivalent, then it is possible for their attitudes to be influenced by a consolidation of elite opinion around a topic. However, so long as culture remains heterogeneous, it seems likely that large groups of the population will vacillate.

These results have implications for methods involving attitudes. The high degree of inconsistency in opinions -- inconsistency that seemingly cannot be attributed to measurement error -- should caution researchers away from assuming that a cross-sectional measure of attitudes is a good proxy for a person's belief. For many questions, more than half a sample might be responding with a temporary attitude construct, while others report real attitudes that matter to them. Given this heterogeneity, we should not expect strong predictive ability for that attitude in general, but that tells us nothing about how it influences the people who truly hold that attitude. "Attitude effects" are going to be shaped by what proportion of the population really holds an atittude, the distribution of stable attitude holds in the population, and the actual effect of the attitude on behavior. At the same time, it is not clear how many respondents need to be making stable responses for an attitude to be a good predictor for behavior (a coarsened version of the question Vaisey [-@vaisey2009] used to predict behavior was only stable for about 27 percent of respondents). 

Ultimately, these results suggest that researchers should devote attention to the social conditions that facilitate stable attitudes and dispositions, whether they are located in people's past or in their contemporary environment. While we know that political awareness tends to facilitate stability in political beliefs, we have no expectation that political awareness should facilitate stability beliefs about religion, morality, or more, but it does suggest that domain-specific awareness might predict domain-specific stability. Attention to the mechanisms that create attitudes can help researchers develop a better understanding of the role of culture in behavior.

# Appendix A: Model Estimation

The following is a summary of the model outlined in Hill and Kreisi [@hill2001b] with modifications for three waves, rather than four. The goal of the Data Augmentation algorithm is to get draws from the posterior distribution $p(\theta|Z)$. It has two main steps: Drawing group membership indicators, given the parameter, and drawing parameters given the group membership indicators.

## Re-Expressing the Data

Rather than estimate parameters on the basis of full patterns, such as "agree"-"disagree"-"strongly disagree," I characterize a respondent by a limited number of general variables. These variable retain the features of the data without having to generate a probability for each pattern separately. For example, by re-expressing the data in this manner I collapse the distinction between someone who says "agree"-"strongly agree"-"agree" and someone who says "agree"-"agree"-"strongly agree." Both patterns become someone who remains on the same side of an issue and gives one strong response.

\begin{align*}
A_i&=\begin{cases}
  1 & \text{is the }i\text{th person's initial response is a 4 or 5}\\
  0 & \text{otherwise}\\
  \end{cases} \\
B_i&=\text{number of the }i\text{th individual's responses that are either 1 or 5 across all *t*} \\
C_i&=\text{number of the }i\text{th individual's responses that are 3} \\
D_i&=\text{number of times the }i\text{th individual crosses an opinion boundary} \\
E_i&=\begin{cases}
  0 & \text{if } D_i \neq 1\\
  t_i^* & \text{otherwise}\\
  \end{cases} \\
F_i&=\begin{cases}
  0 & \text{if the }i\text{th individual's preswitch response is a 1, 2, or 3, or } D_i \neq 1\\
  1 & \text{if the }i\text{th individual's preswitch response is a 4 or 5}\\
  \end{cases} \\
H_i&=\begin{cases}
  0 & \text{if the }i\text{th individual's preswitch response is a 1, 2, 4, or 5, or } D_i \neq 1\\
  1 & \text{if the }i\text{th individual's preswitch response is a 3}\\
  \end{cases} \\
M_i&=\begin{cases}
  0 & \text{if the }i\text{th individual's postswitch response is a 1, 2, or 3, or } D_i \neq 1\\
  1 & \text{if the }i\text{th individual's postswitch response is a 4 or 5}\\
  \end{cases} \\
Q_i&=\begin{cases}
  0 & \text{if the }i\text{th individual's postswitch response is a 1, 2, 4, or 5, or } D_i \neq 1\\
  1 & \text{if the }i\text{th individual's postswitch response is a 3}\\
  \end{cases} \\
R_i&=\text{number of the }i\text{th individual's responses that are either 1 or 2 across all *t*} \\
\end{align*}

I denote the vector of these random variables for individual i as $Z_i$.

## Drawing Group Indicators Given Parameters

I use the observed data $Z_i$ to determine the probability that a person falls into each behavioral group. The conditional probability of belonging to each behavior group given the observed responses is expressed by the following functions: 

\begin{align*}
p_{1,i}^z& =(\alpha_i^{(1-A_i)}(1-\alpha_1)^{A_i}(1-\delta_1)^{(3-B_i)}\delta_1^{B_i})I(C_i=0)I(D_i=0) \\
p_{2,i}^z& =\varphi_2^{C_i}\alpha_2^{R_i}(1-\varphi_2-\alpha_2)^{(3-C_i-R_i)}\delta_2^{B_i}(1-\delta_2)^{(3-C_i-B_i)} \\
p_{3,i}^z& =[\tau_3^{I_{(E_i=1)}}(1-\tau_3)^{I_{(E_i=2)}}({\varphi_3^{(pre1)}}^{H_i}{\alpha_3^{(pre1)}}^{(1-F_i)(1-H_i)}(1-\alpha_3^{(pre1)}-\varphi_3^{(pre1)})^{F_i})^{I_{(E_i=1)}} \\
&   \times ({\varphi_3^{(pre2)}}^{H_i}{\alpha_3^{(pre2)}}^{(1-F_i)(1-H_i)}(1-\alpha_3^{(pre2)}-\varphi_3^{(pre2)})^{F_i})^{I_{(E_i=2)}} \\
&   \times ((1-\alpha_3^{(post)})^{M_i}(\alpha_3^{(post)})^{(1-M_i)})^{H_i}\delta_3^{B_i}(1-\delta_3)^{(3-C_i-B_i)}]I(D_i=1)I(Q_i=0) \\
\end{align*}

where $I(.)$ equals 1 if the condition in the parentheses is met and 0 otherwise. These probabilities represent the expected frequency of a particular pattern given the parameters that describe behavior in that opinion behavior group. For example, if a person responds "no opinion" or "neither agree nor disagree" in one wave, then $D_i \neq 0$, and $p_{1,i}^z$ will equal 0 for that person, since people who express "no opinion" can not be opinion holders. If $\alpha_1=.5$ and $\delta_1=.75$, we expect patterns where a person give a "strongly disagree" response in all three waves to make up about 21 percent of responses in group 1.

Generating these probabilities require parameter estimates. In the first iteration of the model, these parameters are drawn from a random place in the parameter space. 

With these probabilities and draws for $\pi_1$, $\pi_2$, and $\pi_3$ (sampled randomly from the parameter space in the first iteration), I sample from the following trinomial distribution for each person to generate group membership indicators for each iteration. 

\begin{equation}
p(G_i|\theta,Z_i)=\text{Mult}\left(\frac{\pi_1p_{1,i}^z}{\sum_{j=1}^3\pi_jp_{j,i}^z},\frac{\pi_2p_{2,i}^z}{\sum_{j=1}^3\pi_jp_{j,i}^z},\frac{\pi_3p_{3,i}^z}{\sum_{j=1}^3\pi_jp_{j,i}^z}\right)
\end{equation}

These draws generate group membership indicators for a single iteration of the model. The probability that a person's responses are generated from a particular pattern are a function of the likelihood that a pattern was generated by a particular behavioral group, $p_{i,j}$, and the overall prevalence of that behavioral group in the population, $\pi_j$. While a pattern "strongly agree"-"agree"-"strongly agree" might be most consistent with a stable opinion, as the proportion of the population vacillating increases, the the probability that the vacillating group produced this pattern increases.

## Drawing Parameters Given Group Indicators

I then draw parameter estimates from their distribution conditioning on the data ($Z_i$) and the group indicators drawn in the prior step. In other words, I fit separate models for each (temporarily assigned) behavioral group to generate parameter estimates for that iteration.

I use Beta and Dirichlet distributions for prior distributions for each parameter. Assuming a priori independence of the appropriate parameters, I factor $p(\theta)$ into six independent Beta distributions and four independent Dirichlet distributions.[^strength] Parameters are then drawn from the appropriate posterior distributions, using only people in that particular behavioral group, listed below: 

\begin{align*}
p(\alpha_1, 1-\alpha_1)&=\text{Beta}[(N - \sum_{i=1}^N A_i + 1), (\sum_{i=1}^N A_i + 1)] \\
p(\delta_1, 1-\delta_1)&=\text{Beta}[(\frac{\sum_{i=1}^N B_i}{3} + 1), (N - \frac{\sum_{i=1}^N B_i}{3} + 1)] \\
p(\varphi_2, \alpha_2, 1-\varphi_2-\alpha_2)&=\text{Dirichlet}[(\frac{\sum_{i=1}^N C_i}{3} + \frac{2}{3}),\\
&(\frac{\sum_{i=1}^N R_i}{3} + \frac{2}{3}), \\
&(N - \frac{\sum_{i=1}^N C_i}{3} + \frac{2}{3} - \frac{\sum_{i=1}^N R_i}{3} + \frac{2}{3})] \\
p(\delta_2, 1-\delta_2)&=\text{Beta}[(\frac{\sum_{i=1}^N B_i}{3} + \frac{2}{3}), (N - \frac{\sum_{i=1}^N B_i}{3} + \frac{2}{3})] \\
p(\varphi_3^{(pre1)}, \alpha_3^{(pre1)}, 1-\varphi_3^{(pre1)}-\alpha_3^{(pre1)})&= \text{Dirichlet}[
(\sum_{i=1}^N H_i I(E_i=1) + \frac{2}{6}), \\
&(N^{(E_i=1)} - \sum_{i=1}^N F_i I(E_i=1) - \sum_{i=1}^N H_i I(E_i=1) + \frac{2}{6}), \\
&(\sum_{i=1}^N F_i I(E_i=1) + \frac{2}{6})] \\
p(\varphi_3^{(pre2)}, \alpha_3^{(pre2)}, 1-\varphi_3^{(pre2)}-\alpha_3^{(pre2)})&= \text{Dirichlet}[
(\sum_{i=1}^N H_i I(E_i=2) + \frac{2}{6}), \\
&(N^{(E_i=1)} - \sum_{i=1}^N F_i I(E_i=2) - \sum_{i=1}^N H_i I(E_i=2) + \frac{2}{6}), \\
&(\sum_{i=1}^N F_i I(E_i=2) + \frac{2}{6})] \\
p(\alpha_3^{(post)}, 1-\alpha_3^{(post)})&=\text{Beta}[(\sum_{i=1}^N H_i - \sum_{i=1}^N M_i I(H_i=1) + \frac{2}{6}), (\sum_{i=1}^N M_i I(H_i=1) + \frac{2}{6})] \\
p(\delta_3, 1-\delta_3)&=\text{Beta}[(\frac{\sum_{i=1}^N B_i}{3} + \frac{2}{3}), (N - \frac{\sum_{i=1}^N B_i}{3} + \frac{2}{3})] \\
p(\tau_3, 1-\tau_3)&=\text{Beta}[(\sum_{i=1}^N I(E_i = 1) + 1), (\sum_{i=1}^N I(E_i = 2) + 1)] \\
p(\pi_1, \pi_2, \pi_3)&=\text{Dirichlet}[(\sum_{i=1}^N G_i=1 + 2), (\sum_{i=1}^N G_i=2 + 2), (\sum_{i=1}^N G_i=3 + 2)] \\
\end{align*}

[^strength]: In the seven-point-scale model, $\delta$ and $\gamma$ estimates are drawn from a Dirichlet distribution (one for each behavioral group), rather than the Beta distribution for the $\delta$ estimate in the five-point-scale model. In the three-point-scale model, no $\delta$ parameter is estimated.

The formulas look complicated, but they amount to counting the number of response patterns that follow the particular rule the parameter pertains to. This is most obvious in the case of the $pi_j$ estimates, which are a Dirichlet draw from the count of each person assigned to that group for the current iteration. Similarly, the estimate for $alpha_1$, $(N - \sum_{i=1}^N A_i + 1)$, is simply the total number of people in Group 1 (opinion holders), $N$, minus the number of people who disagree, $\sum_{i=1}^N A_i$, plus 1 (the prior). Since opinion holders can only agree or disagree, this gives the count of people who agree, which is what $\alpha_1$ indicates.

As noted above the priors are functionally equivalent to adding two people to each behavior group and splitting up their behavior within those groups. Hill and Kreisi find that changes in these priors have minimal effects on the actual estimates. The greatest instability is observed in the durable changers group, which is often small and therefore more susceptible to the influence priors.

## Convergence 

The model iterates until it reaches a stable distribution, from which draws were taken to capture the posterior distribution. Functionally, I used five chains, each with 2500 iterations. I discarded the first 500 iterations to eliminate estimates generated prior to convergence. Diagnostics suggest that, for most questions, parameter estimates converged well before the 500th iteration. 


# Appendix B: Questions

The table below lists the 544 questions analyzed here, actual question wording, how a question was recoded (if at all), the number of response options (2, 3, 4, 5, or 7) included in each model, and the structural and content tags associated with the question. 

```{r, eval = FALSE}
qm %>%
  select(text)
  
```

# Appendix C: Question Content and Structure

Give the large number of questions examined and the covariation in content and structure, it is difficult just looking at the overall patterns to say whether it is question content or structure that drives overall rates of stability or vacillation. The GSS has many more questions with fewer response options (questions with two, three, or four choices make up most of the GSS questions), but also more questions about non-political content. The ANES panels tends to favor questions with more response options, typically five or seven, but focuses almost exclusively on questions of politics. An additional feature of question structure that might matter is the explicit option of a midpoint to signal ambivalence.

To better assess these influences, I tag questions with broad indicators for content areas: government and politics; civil liberties; medicine and science; race; sex; family and gender[^famgen]; socioeconomic position; work; morality; foreign relations; self-evaluations; religion; crime; social trust; and other. These categories are not mutually exclusive, and many questions are asked in a survey because feature the intersection of multiple domains. Because of this, I focus on an explicit reading of the question and code items parsimoniously. Because more than half of all questions analyzed here touch on government and politics in some capacity, I instead use an indicator to signal that a question does *not* explicitly deal with politics. I also include indicators for whether a question asks people to assess another person's position, and indicators for whether the question refers to a clear group of people or a particular person.

[^famgen]: Ideally I would include separate indicators for questions relating to gender and questions relating to family structures. However, almost all questions about gender touch on some other subject material, with family structures being the largest overlap. 

I also create indicator variables for question structure. First, I include five indicators about question format. The first format asks whether people agree with a statement. These include traditional five- and four-point Likert scales and two- and three-point agree/disagree or yes/no statements. The second format asks people to select a position select between two poles or options. The third format, common in the ANES panels, asks respondents to place a respondent on a "feeling thermometer." A fourth format asks people whether groups have "too much" influence in politics. The final category includes other question structures. Most questions in this last category evaluate the intensity of something, such as whether an amount should increase, stay the same, or decrease. While many questions with "other" structures were collapsed to reflect a "yes/no" structure, I code based on the original wording. I also include indicator variables for the number of response options (2, 3, 4, 5, 7) and indicator variables for data set. 

I then regress the proportion of respondents falling into each behavioral group on these indicators. As noted above, the questions explored here do not represent a random sample of questions, so standard confidence intervals and statistical inferences are not meaningful. However, I incorporate uncertainty in two ways. First, I bootstrap coefficient estimates, randomly sampling questions with replacement 10,000 times. Second, to incorporate the uncertainty in each question's proportion estimate, each time a parameter is sampled in the bootstrap, I draw a random value from that parameter's posterior distribution. By bootstrapping the estimates we can be relatively confident that whatever patterns we observe are not driven by a single question or small group of questions with unusual behavior, even if I cannot generalize to all attitudes.

In all regressions, the reference group consists of general questions about government (particularly government's role in the domestic economy) on a five-point agree/disagree scale, with the General Social Survey serving as the reference data set. 

## Vacillation

Figure \ref{fig:vacboot} presents coefficient estimates for a regression of the proportion of respondents giving vacillating response patterns on question content and structure. An unpictured regression of the proportion of respondents giving durable change response patterns on the same parameters is almost a mirror-image of the figure presented here, with the variables predicting higher vacillation predicting lower stability. As a result, I will discuss both stability and vacillation in reference to the Figure \ref{fig:vacboot}. 

```{r vacboot, fig.cap='Coefficient estimates of regression of proportion of respondents of giving a vacillating response on question content and structure indictors. Confidence intervals generated from 10,000-iteration bootstrap.', fig.align='center'}
load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/pi2_boot.Rdata")
pi2_boot %>%
  gather(key = "param", value = "value") %>%
  group_by(param) %>%
  summarise(mean = mean(value, na.rm = TRUE), 
            q25=quantile(value, .025, na.rm = TRUE), 
            q975=quantile(value, .975, na.rm = TRUE)) %>%
  mutate(group = ifelse(param %in% c("choice2", "choice3", "choice4", "choice7",
                                     "ft", "infl", "other", "bipolar", "anes5",
                                     "anes7", "anes9", "anes2k", "lsg",
                                     "nsyr", "safc", "safm", "cces"), "Question Structure",
                        "Question Content")) %>%
  filter(param != "(Intercept)") %>%
  mutate(param = recode(param, "other_assess"="Assess another", "civlibs"="Civil liberties",
                        "moral"="Morality", "foreign"="Foreign policy/military",
                        "not_politics"="Not politics", "group"="Subject: Group",
                        "medsci"="Medicine/science", "ses"="SES", "famgndr"="Family/gender",
                        "work"="Work", "race"="Race", "person"="Subject: Person",
                        "crime"="Crime", "trust"="Social Trust", "self"="Self-eval.",
                        "religion"="Religion", "sex"="Sex/sexuality",
                        "oth_content"="Other content",
                        "other"="Structure: Other", "bipolar"="Structure: Bipolar",
                        "ft"="Structure: Feeling therm.", "infl"="Structure: Influence",
                        "choice7"="Choices: 7", "choice4"="Choices: 4", "choice3"="Choices: 3",
                        "choice2"="Choices: 2",
                        "gss"="GSS (2006-2014)", "safm"="ISPC-Mom (1980-1993)",
                        "safc"="ISPC-Child (1980-1993)", "lsg"="LSG (varies)", 
                        "nsyr"="NSYR (varies)", "anes7"="ANES (1972-1976)",
                        "anes5"="ANES (1956-1960)", "anes9"="ANES (1992-1996)",
                        "anes2k"="ANES (2000-2004)", "cces"="CCES (2010-2014)")) %>%
  ggplot(aes(x = reorder(param, mean), y = mean,
             fill = group)) + 
  geom_hline(yintercept = 0) + 
  geom_linerange(aes(ymin = q25, ymax = q975)) + 
  geom_point(shape = 21) + 
  facet_wrap(~group, scales = "free_y") + 
  coord_flip() + 
  theme_bw() +
  theme(legend.position = "none") + 
  labs(x = "", y = "Coefficient estimate")

```

A general five-point agree/disagree question about domestic government and politics from the GSS has an expected proportion of people vacillating of about .58 with a standard error of .033. Two content areas demonstrate notably less vacillation than other kinds of questions: questions about religious beliefs and questions about sex and homosexuality. On the other end of the scale, questions about general morality, questions about civil liberties, and questions about medicine and science show more vacillation than other kinds of questions. When questions fall outside of the political space ("Not politics" in Figure \ref{fig:vacboot}), they tend to show higher levels of vacillation. Notably, questions about race, foreign relations, work, crime, and family and gender do not appear substantially different from questions about politics once I control for question structure. 

Questions that ask people to assess others -- where do politicians or parties fall on particular issue scales -- demonstrate much higher vacillation than questions about people's own opinions. This is consistent with previous work [@alwin2007] and suggests a lack of knowledge about actors' in the political sphere.

In terms of question structure, stability decreases and vacillation increases as the number of response options increases. This helps explain the higher levels of stability for GSS and CCES questions, which have fewer response options on average, relative to the other data sets. Questions about groups' political influence show more stability than other kinds of questions, but beyond that, question structure does not seem to matter much.

Finally, even controlling for question structure, data sets demonstrate different rates of vacillation. Notably, the 1972-1976 ANES and the NSYR demonstrate more vacillation than other data sets. The CCES demonstrates lower rates of vacillation.

### Durable Change 

Figure \ref{fig:durableboot} plots coefficient estimates of the proportion of respondents demonstrating durable change. The coefficients associated with question content and structure are much smaller on durable change than on vacillating change, reflecting its truncation in low end of the proportion rage, and the parameters here do not explain nearly as much variance as the model for vacillating change. There is almost no appreciable effect of question content on the rate of durable change.

```{r durableboot, fig.cap='Coefficient estimates of regression of proportion of respondents of giving a durable change response on question content and structure indictors. Confidence intervals generated from 10,000-iteration bootstrap.', fig.align='center'}

load("~/Dropbox/hill_kreisi/results/ambivalence_everywhere_results/pi3_boot.Rdata")

pi3_boot %>%
  gather(key = "param", value = "value") %>%
  group_by(param) %>%
  summarise(mean = mean(value, na.rm = TRUE), 
            q25=quantile(value, .025, na.rm = TRUE), 
            q975=quantile(value, .975, na.rm = TRUE)) %>%
  mutate(group = ifelse(param %in% c("choice2", "choice3", "choice4", "choice7",
                                     "ft", "infl", "other", "bipolar", "anes5",
                                     "anes7", "anes9", "anes2k", "lsg",
                                     "nsyr", "safc", "safm", "cces"), "Question Structure",
                        "Question Content")) %>%
  filter(param != "(Intercept)") %>%
  mutate(param = recode(param, "other_assess"="Asses another", "civlibs"="Civil liberties",
                        "moral"="Morality", "foreign"="Foreign policy/military",
                        "not_politics"="Not politics", "group"="Subject: Group",
                        "medsci"="Medicine/science", "ses"="SES", "famgndr"="Family/gender",
                        "work"="Work", "race"="Race", "person"="Subject: Person",
                        "crime"="Crime", "trust"="Social Trust", "self"="Self-eval.",
                        "religion"="Religion", "sex"="Sex/sexuality",
                        "oth_content"="Other content",
                        "other"="Structure: Other", "bipolar"="Structure: Bipolar",
                        "ft"="Structure: Feeling therm.", "infl"="Structure: Influence",
                        "choice7"="Choices: 7", "choice4"="Choices: 4", "choice3"="Choices: 3",
                        "choice2"="Choices: 2",
                        "gss"="GSS (2006-2014)", "safm"="ISPC-Mom (1980-1993)",
                        "safc"="ISPC-Child (1980-1993)", "lsg"="LSG (varies)", 
                        "nsyr"="NSYR (varies)", "anes7"="ANES (1972-1976)",
                        "anes5"="ANES (1956-1960)", "anes9"="ANES (1992-1996)",
                        "anes2k"="ANES (2000-2004)", "cces"="CCES (2010-2014)")) %>%
  ggplot(aes(x = reorder(param, mean), y = mean,
             fill = group)) + 
  geom_hline(yintercept = 0) + 
  geom_linerange(aes(ymin = q25, ymax = q975)) + 
  geom_point(shape = 21) + 
  facet_wrap(~group, scales = "free_y") + 
  coord_flip() + 
  theme_bw() +
  theme(legend.position = "none") + 
  labs(x = "", y = "Coefficient estimate")
```

Two data sets demonstrate substantially higher levels of durable change: the NSYR and the ISPC-Child, the two panels with the lowest average age.

There is a less clear pattern on question structure for durable change than for vacillating change. Questions with five response options (the reference category) demonstrate the most durable change, followed by questions with three responses. Questions with two, four, and seven responses demonstrate comparable, low levels of durable change.

In addition to the Nixon question discussed above, other questions with high rates of durable change include a shift in the NSYR in which many adolescents go from saying that people should wait to have sex until they are married to saying "not necessarily," with very few shifting the other way, and a large change of opinion around whether the presidential election in 2000 was fair. 


\newpage

# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent

